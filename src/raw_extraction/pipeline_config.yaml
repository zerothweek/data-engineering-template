# --- Global Settings ---
storage:
  base_path: "../../data_lake/raw_data"
  # Default output format: 'parquet' is recommended for big data
  # Options: 'parquet', 'csv'
  default_format: "parquet" 
  # Compression Settings
  # Note: This only applies to 'parquet' format. 
  # CSV output is always uncompressed (plain text).
  default_compression: "snappy" 

# --- Execution Tuning ---
execution:
  # Rows to process at a time. 
  chunk_size: 50000 

# --- Connection Registry ---
# Define reusable database connections here.
# SECURITY NOTE: Do NOT put actual passwords here. 
# Provide the NAME of the environment variable that holds the password.
connections:
  mysql_db_sakila:
    drivername: "mysql+pymysql"
    username: "root"
    password_env_var: "MYSQL_ROOT_PASSWORD" # The script will look for os.environ['MYSQL_ROOT_PASSWORD']
    host: "example-mysql-db-network"
    port: 3306
    database: "sakila"


# --- Extraction Jobs ---
jobs:
  late_return_prediction:
    connection: "mysql_db_sakila" # References the connection above
    description: "raw data for Predicting if a customer will return a DVD late."
    # Option A: Inline Query (Good for simple selects)
    #query: "SELECT * FROM actor"
    query_file: "queries/late_return_prediction.sql" 
    tags: ["ml", "dvd_late_return_pred", "csv_conversion_test"]
    
